# @package _global_
action_name: _3_latent_separation_score
debug: false

action_config:
  experiment_name: Latent Separation Score calculation
  device: cuda # auto, cuda, cpu (default)
  seed: 73

  task:
    model:
      type:
        _target_: transformer_lens.HookedTransformer.from_pretrained
      name: google/gemma-2-2b
      # name: google/gemma-2-9b

    sae:
      type:
        _target_: sae_lens.SAE.from_pretrained
      # release: gpt2-small-res-jb
      release: gemma-scope-2b-pt-res-canonical
      # release: gemma-scope-9b-pt-res-canonical
      # skip every other layer
      skip_layers: true
      width: width_16k

    # replace the TODO with the actual path of the data file
    dataset:
      type:
        _target_: urartu.datasets.DatasetFromFile
      # google/gemma-2-2b
      data_files: .runs/_2_sample_constructor/TODO/template2_balance_c4f58bc1/template2_balance_c4f58bc1_type

      # google/gemma-2-9b
      # data_files: .runs/_2_sample_constructor/TODO/template2_balance_c4f58bc1/template2_balance_c4f58bc1_type

      file_extension: jsonl
      dataloader:
        input_key: "template"
        output_key: "attribute"
        batch_size: 64
        shuffle: false

    # plot top_k activations on each layer
    top_k: 5

    # cls_token_index: "entity_last"
    cls_token_index: -1
    # options:
    #  entity_last: the last token of the entity as the cls
    #  *any integer, e.g.:
    #   -1: the last token of the prompt
    #   -2: the token before the last token of the prompt

    # --- disabling this as it's redundent for template2 ---
    # ignore_of: false
    # # !!! works with "cls_token_index: int"
    # # we have these type of relations in the dataset:
    # # 1 - "was born in the city of"
    # # 2 - "was born in year"
    # # trivially for 2nd sample the last token will be the "year" while for 1st sample the "of" instead of the "city".
# hydra:
#   sweeper:
#     params:
#       # action_config.task.k: 5,10
#       # action_config.seed: 12,937,1286
